# -*- coding: utf-8 -*-
"""Formal_Project_3_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VgmP1F06iMmWvfeOxxX5VxPHbDIrRmxn
"""

#importing necessary for excel dataset upload
import pandas as pd
import numpy as np
from google.colab import files
import io
uploaded = files.upload()
df1 = pd.read_csv(io.StringIO(uploaded['data1.csv'].decode('utf-8')),header=None)
#df1.sample(frac=1)

df1 = df1.sample(frac=1)

z = df1.to_numpy()[1:,:]
z

#Debug step
y = z[:,421]
y

from keras.utils import to_categorical

#Splitting Dataset into train and test
#Assuming 1000 entries
##200 time steps in each row

arr_df1 = z.copy()
#80-20 train-test split
train = arr_df1[:150,:].copy()
test = arr_df1[120:150,:].copy()

train_x = train[:,:421].copy()
train_x = (train_x-np.min(train_x))/(np.max(train_x)-np.min(train_x))
test_x = test[:,:421].copy()
train_y = train[:,421].copy()
test_y = test[:,421].copy()

##One-hot encoding

#train_y = to_categorical(train_y)
#test_y = to_categorical(test_y)

##Reshaping data

train_x = train_x.reshape(train_x.shape[0],train_x.shape[1],1)
test_x = test_x.reshape(test_x.shape[0],test_x.shape[1],1)

# cnn model
from numpy import mean
from numpy import std
from numpy import dstack
from pandas import read_csv
from matplotlib import pyplot
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.utils import to_categorical

import keras
from keras.models import Model
from keras.models import Sequential
from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense

##Important parameters

epochs = 100
input_s= (train_x.shape[1],1)

##Assume each row is 420 time steps 
#Sample-Current-Model 
#I/P-->1DConvLayer-->BatchNormalization-->MaxPool-->1DConvLayer-->BatchNormalization-->MaxPool-->Flattenlayer-->Dense-->Dense-->Dense-->Dense-->O/p

model = Sequential()

model.add(Conv1D(filters=32, kernel_size=32, activation='relu',input_shape=(train_x.shape[1],1)))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=(2)))
model.add(Conv1D(filters=32, kernel_size=8, activation='sigmoid'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=(2)))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
#model.add(Dropout(0.15))
model.add(Dense(32, activation='sigmoid'))
#model.add(Dropout(0.075))
model.add(Dense(16, activation='relu'))
#model.add(Dropout(0.035))
model.add(Dense(8, activation='sigmoid'))
#model.add(Dropout(0.01))
model.add(Dense(1,activation='relu'))
###End of model definition
print(model.summary())
#Set loss function
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


##Training and fitting the model
model.fit(train_x, train_y,epochs=epochs,verbose=1)

##Testing the model
_, accuracy = model.evaluate(test_x, test_y, batch_size=batch_size, verbose=1)
print(accuracy)

_, accuracy = model.evaluate(test_x, test_y, batch_size=batch_size, verbose=1)
print(accuracy)

test_y

